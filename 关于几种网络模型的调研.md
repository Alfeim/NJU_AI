1.AlexNet
  CNN的经典案例
  
  使用了非线性激活函数ReLU
  
  提出了LRN（Local Response Normalization），局部响应归一化，LRN一般用在激活和池化函数后，对局部神经元的活动创建竞争机制，使其中响应比较大对值变得相对更
  大，并抑制其他反馈较小的神经元，增强了模型的泛化能力
  
  使用CUDA加速深度神经卷积网络的训练，利用GPU强大的并行计算能力，处理神经网络训练时大量的矩阵运算

  在CNN中使用重叠的最大池化，AlexNet全部使用最大池化，避免平均池化的模糊化效果。AlexNet中提出让步长比池化核的尺寸小，这样池化层的输出之间会有重叠和覆盖，
  提升了特征的丰富性。
  
  使用数据增广（data agumentation）和Dropout防止过拟合。
  
  随机地从256*256的原始图像中截取224*224大小的区域，相当于增加了2048倍的数据量；
  
  Dropout  AlexNet在后面的三个全连接层中使用Dropout，随机忽略一部分神经元，以避免模型过拟合

2.VGG Net
  VGG全部使用了3*3的卷积核和2*2最大池化核通过不断加深网络结构来提神性能。采用堆积的小卷积核优于采用大的卷积核，因为多层非线性层可以增加网络深层来保证学习更
  复杂的模式，而且所需的参数还比较少.
  
3.GoogleNet
  
  提高深度神经网络性能最直接的办法是增加它们尺寸，不仅仅包括深度（网络层数），还包括它的宽度，即每一层的单元个数。但是这种简单直接的解决方法存在两个重大
  的缺点，更大的网络意味着更多的参数，使得网络更加容易过拟合，而且还会导致计算资源的增大。经过多方面的思量，考虑到将稀疏矩阵聚类成相对稠密子空间来倾向于
  对稀疏矩阵的优化，因而提出inception结构。Google Inception Net是一个大家族

  GoogleNet的几个分支:待填

4.ResNet
  
  随着网络的加深，出现了训练集准确率下降，错误率上升的现象，就是所谓的“退化”问题。按理说更深的模型不应当比它浅的模型产生更高的错误率，这不是由于过拟
  合产生的，而是由于模型复杂时，SGD的优化变得更加困难，导致模型达不到好的学习效果。ResNet就是针对这个问题应运而生的。比较适合复杂(深层)神经网络.
  
 
  在微痕检测应用上,目前来看,如果对于输入处理得当(放大特征、减少冗余特征空间)可以简化网络复杂度,因此可能直接使用AlexNet反而是最方便、快捷的一种方式
  如果对于准确度有较高要求,也可以尝试GoogleNet.
 
  如果模式比较复杂可能需要考虑借助VGG或是ResNet


  
  
